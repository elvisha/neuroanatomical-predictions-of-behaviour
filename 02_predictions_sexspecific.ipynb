{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys; sys.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, GroupKFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean HCP data\n",
    "# set base dirctories\n",
    "HCP_base_dir   = '/Users/elvishadhamala/Documents/yale/HCP/'\n",
    "\n",
    "# load dataframes for the cortical areal-level properties\n",
    "# suraface area, gray matter volume, and cortical thickness\n",
    "HCP_surf_area_df = pd.read_csv(os.path.join(HCP_base_dir, 'group_net_surfarea_native_freesurfer.csv'))\n",
    "HCP_gray_vol_df = pd.read_csv(os.path.join(HCP_base_dir, 'group_net_grayvol_native_freesurfer.csv'))\n",
    "HCP_thick_df = pd.read_csv(os.path.join(HCP_base_dir, 'group_net_thickavg_native_freesurfer.csv'))\n",
    "\n",
    "# load subj behavioural and family data\n",
    "HCP_subj_data_df = pd.read_csv(os.path.join(HCP_base_dir, 'hcp_behaviour.csv'))\n",
    "HCP_subj_fam_df = pd.read_csv(os.path.join(HCP_base_dir, 'hcp_restricted.csv'))\n",
    "HCP_subj_fs_df = pd.read_csv(os.path.join(HCP_base_dir, 'hcp_freesurfer.csv'))\n",
    "\n",
    "#create a mask for the subject and family data and only keep the ones that have the cortical areal-level properties \n",
    "HCP_mask_data=np.isin(HCP_subj_data_df.Subject, HCP_surf_area_df.id)\n",
    "HCP_subj_data_df = HCP_subj_data_df[HCP_mask_data]\n",
    "\n",
    "HCP_mask_fam=np.isin(HCP_subj_fam_df.Subject, HCP_surf_area_df.id)\n",
    "HCP_subj_fam_df = HCP_subj_fam_df[HCP_mask_fam]\n",
    "\n",
    "HCP_mask_fs=np.isin(HCP_subj_fs_df.Subject, HCP_surf_area_df.id)\n",
    "HCP_subj_fs_df = HCP_subj_fs_df[HCP_mask_fs]\n",
    "\n",
    "\n",
    "HCP_surf_area_df = HCP_surf_area_df.set_index(HCP_subj_data_df.index)\n",
    "HCP_gray_vol_df = HCP_gray_vol_df.set_index(HCP_subj_data_df.index)\n",
    "HCP_thick_df = HCP_thick_df.set_index(HCP_subj_data_df.index)\n",
    "HCP_subj_fs_df = HCP_subj_fs_df.set_index(HCP_subj_data_df.index)\n",
    "\n",
    "\n",
    "HCP_icv = pd.DataFrame(HCP_subj_fs_df.FS_InterCranial_Vol)\n",
    "\n",
    "# drop the id and 'none' columns\n",
    "HCP_surf_area_df = HCP_surf_area_df.drop(columns=['id', 'lh_None', 'rh_None'])\n",
    "HCP_gray_vol_df = HCP_gray_vol_df.drop(columns=['id', 'lh_None', 'rh_None'])\n",
    "HCP_thick_df = HCP_thick_df.drop(columns=['id', 'lh_None', 'rh_None'])\n",
    "\n",
    "\n",
    "\n",
    "HCP_cog = HCP_subj_data_df[[\"CogFluidComp_Unadj\", \"CogEarlyComp_Unadj\", \"CogTotalComp_Unadj\", \"CogCrystalComp_Unadj\",\n",
    "                  \"PicSeq_Unadj\", \"CardSort_Unadj\", \"Flanker_Unadj\", \"PMAT24_A_CR\", \"ReadEng_Unadj\", \n",
    "                  \"PicVocab_Unadj\", \"ProcSpeed_Unadj\", \"DDisc_AUC_40K\", \"VSPLOT_TC\", \"SCPT_SEN\", \"SCPT_SPEC\", \n",
    "                  \"IWRD_TOT\", \"ListSort_Unadj\", \"MMSE_Score\", \"Language_Task_Math_Avg_Difficulty_Level\", \n",
    "                  \"Language_Task_Story_Avg_Difficulty_Level\", \"Relational_Task_Acc\", \"WM_Task_Acc\"]] \n",
    "\n",
    "\n",
    "col_headers_main = ['Fluid Composite', 'Early Composite', 'Total Composite',\n",
    "               'Crystal Composite', 'Visual Episodic Memory', 'Cognitive Flexibility (Card Sort)',\n",
    "               'Inhibition (Flanker)', 'Fluid Intelligence (PMAT)', 'Reading Decoding', \n",
    "               'Vocabulary Comprehension', 'Processing Speed', 'Delay Discounting',\n",
    "               'Spatial Orientation', 'Sustained Attention - Sens.', \n",
    "               'Sustained Attention - Spec.', 'Verbal Episodic Memory', \n",
    "               'Working Memory (List Sorting)', 'Cognitive Status', 'Arithmetic',\n",
    "               'Story Comprehension', 'Relational Processing', 'Working Memory (N-Back)']\n",
    "\n",
    "HCP_cog.columns = col_headers_main\n",
    "\n",
    "HCP_cog = HCP_cog[['Fluid Composite', 'Total Composite', 'Crystal Composite',\n",
    "                     'Visual Episodic Memory', 'Cognitive Flexibility (Card Sort)',\n",
    "                     'Inhibition (Flanker)', 'Reading Decoding', 'Vocabulary Comprehension', \n",
    "                     'Processing Speed', 'Working Memory (List Sorting)']]\n",
    "\n",
    "\n",
    "\n",
    "#get rid of all the subjects with nans\n",
    "HCP_mask = np.asarray([~HCP_cog.isna().any(axis=1)])\n",
    "HCP_surf_area = HCP_surf_area_df[np.transpose(HCP_mask==True)]\n",
    "HCP_gray_vol = HCP_gray_vol_df[np.transpose(HCP_mask==True)]\n",
    "HCP_thick = HCP_thick_df[np.transpose(HCP_mask==True)]\n",
    "HCP_icv = HCP_icv[np.transpose(HCP_mask==True)]\n",
    "HCP_cog = HCP_cog[np.transpose(HCP_mask==True)]\n",
    "HCP_fam = HCP_subj_fam_df.loc[np.transpose(HCP_mask==True)]\n",
    "HCP_subj = HCP_subj_data_df.loc[np.transpose(HCP_mask==True)]\n",
    "\n",
    "# get normalised measured (by icv)\n",
    "HCP_surf_area_norm = pd.DataFrame(HCP_surf_area.values/HCP_icv.values, columns=HCP_surf_area.columns)\n",
    "HCP_gray_vol_norm = pd.DataFrame(HCP_gray_vol.values/HCP_icv.values, columns=HCP_gray_vol.columns)\n",
    "HCP_thick_norm = pd.DataFrame(HCP_thick.values/HCP_icv.values, columns=HCP_thick.columns)\n",
    "\n",
    "\n",
    "HCP_surf_area = HCP_surf_area.set_index(HCP_subj.index)\n",
    "HCP_gray_vol = HCP_gray_vol.set_index(HCP_subj.index)\n",
    "HCP_thick = HCP_thick.set_index(HCP_subj.index)\n",
    "HCP_surf_area_norm = HCP_surf_area_norm.set_index(HCP_subj.index)\n",
    "HCP_gray_vol_norm = HCP_gray_vol_norm.set_index(HCP_subj.index)\n",
    "HCP_thick_norm = HCP_thick_norm.set_index(HCP_subj.index)\n",
    "HCP_fam = HCP_fam.set_index(HCP_subj.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean up ABCD data\n",
    "# set base dirctories\n",
    "ABCD_base_dir   = '/Users/elvishadhamala/Documents/yale/ABCD'\n",
    "\n",
    "# load dataframes for the cortical areal-level properties\n",
    "# suraface area, gray matter volume, and cortical thickness \n",
    "ABCD_surf_area_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_group_surfarea.csv'), header=None)\n",
    "ABCD_gray_vol_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_group_grayvol.csv'), header=None)\n",
    "ABCD_thick_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_group_thickavg.csv'), header=None)\n",
    "ABCD_icv_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_icv.csv'), header=None)\n",
    "\n",
    "# load subj behavioural and family data\n",
    "ABCD_subj = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_1823_demo_cog.csv'))\n",
    "\n",
    "\n",
    "ABCD_surf_area = ABCD_surf_area_df.T\n",
    "ABCD_gray_vol = ABCD_gray_vol_df.T\n",
    "ABCD_thick = ABCD_thick_df.T\n",
    "\n",
    "\n",
    "# get normalised measured (by icv)\n",
    "ABCD_surf_area_norm = pd.DataFrame(ABCD_surf_area.values/ABCD_icv_df.values, columns=ABCD_surf_area.columns)\n",
    "ABCD_gray_vol_norm = pd.DataFrame(ABCD_gray_vol.values/ABCD_icv_df.values, columns=ABCD_gray_vol.columns)\n",
    "ABCD_thick_norm = pd.DataFrame(ABCD_thick.values/ABCD_icv_df.values, columns=ABCD_thick.columns)\n",
    "\n",
    "\n",
    "ABCD_cog = ABCD_subj\n",
    "ABCD_cog = ABCD_cog.drop(columns=['subjectkey', 'src_subject_id', 'sex', 'race_ethnicity', 'site_id_l'])\n",
    "ABCD_cog\n",
    "\n",
    "col_headers_main = ['Vocabulary Comprehension', 'Inhibition (Flanker)', 'Working Memory (List Sorting)',\n",
    "                   'Cognitive Flexibility (Card Sort)', 'Processing Speed', 'Visual Episodic Memory',\n",
    "                   'Reading Decoding', 'Fluid Composite', 'Crystal Composite', 'Total Composite',\n",
    "                   'RAVLT - Trial VI Correct', 'RAVLT - Trial VII Correct', 'WISC-V - Total Raw Score',\n",
    "                   'LMT - % Correct', 'LMT - RT Correct', 'LMT Efficiency']\n",
    "\n",
    "ABCD_cog.columns = col_headers_main\n",
    "\n",
    "ABCD_cog = ABCD_cog[['Fluid Composite', 'Total Composite', 'Crystal Composite',\n",
    "                     'Visual Episodic Memory', 'Cognitive Flexibility (Card Sort)',\n",
    "                     'Inhibition (Flanker)', 'Reading Decoding', 'Vocabulary Comprehension', \n",
    "                     'Processing Speed', 'Working Memory (List Sorting)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask sex for HCP\n",
    "mask_sex = HCP_subj.Gender=='M'\n",
    "HCP_subj_m = HCP_subj[mask_sex]\n",
    "HCP_surfarea_m = HCP_surf_area[mask_sex]\n",
    "HCP_grayvol_m = HCP_gray_vol[mask_sex]\n",
    "HCP_thick_m = HCP_thick[mask_sex]\n",
    "HCP_surfarea_norm_m = HCP_surf_area_norm[mask_sex]\n",
    "HCP_grayvol_norm_m = HCP_gray_vol_norm[mask_sex]\n",
    "HCP_thick_norm_m = HCP_thick_norm[mask_sex]\n",
    "HCP_fam_m = HCP_fam[mask_sex]\n",
    "HCP_cog_m = HCP_cog[mask_sex]\n",
    "\n",
    "\n",
    "mask_sex = HCP_subj.Gender=='F'\n",
    "HCP_subj_f = HCP_subj[mask_sex]\n",
    "HCP_surfarea_f = HCP_surf_area[mask_sex]\n",
    "HCP_grayvol_f = HCP_gray_vol[mask_sex]\n",
    "HCP_thick_f = HCP_thick[mask_sex]\n",
    "HCP_surfarea_norm_f = HCP_surf_area_norm[mask_sex]\n",
    "HCP_grayvol_norm_f = HCP_gray_vol_norm[mask_sex]\n",
    "HCP_thick_norm_f = HCP_thick_norm[mask_sex]\n",
    "HCP_fam_f = HCP_fam[mask_sex]\n",
    "HCP_cog_f = HCP_cog[mask_sex]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask sex for ABCD\n",
    "\n",
    "mask_sex = ABCD_subj.sex=='M'\n",
    "ABCD_subj_m = ABCD_subj[mask_sex]\n",
    "ABCD_surfarea_m = ABCD_surf_area[mask_sex]\n",
    "ABCD_grayvol_m = ABCD_gray_vol[mask_sex]\n",
    "ABCD_thick_m = ABCD_thick[mask_sex]\n",
    "ABCD_surfarea_norm_m = ABCD_surf_area_norm[mask_sex]\n",
    "ABCD_grayvol_norm_m = ABCD_gray_vol_norm[mask_sex]\n",
    "ABCD_thick_norm_m = ABCD_thick_norm[mask_sex]\n",
    "ABCD_cog_m = ABCD_cog[mask_sex]\n",
    "\n",
    "mask_sex = ABCD_subj.sex=='F'\n",
    "ABCD_subj_f = ABCD_subj[mask_sex]\n",
    "ABCD_surfarea_f = ABCD_surf_area[mask_sex]\n",
    "ABCD_grayvol_f = ABCD_gray_vol[mask_sex]\n",
    "ABCD_thick_f = ABCD_thick[mask_sex]\n",
    "ABCD_surfarea_norm_f = ABCD_surf_area_norm[mask_sex]\n",
    "ABCD_grayvol_norm_f = ABCD_gray_vol_norm[mask_sex]\n",
    "ABCD_thick_norm_f = ABCD_thick_norm[mask_sex]\n",
    "ABCD_cog_f = ABCD_cog[mask_sex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fluid Composite</th>\n",
       "      <th>Total Composite</th>\n",
       "      <th>Crystal Composite</th>\n",
       "      <th>Visual Episodic Memory</th>\n",
       "      <th>Cognitive Flexibility (Card Sort)</th>\n",
       "      <th>Inhibition (Flanker)</th>\n",
       "      <th>Reading Decoding</th>\n",
       "      <th>Vocabulary Comprehension</th>\n",
       "      <th>Processing Speed</th>\n",
       "      <th>Working Memory (List Sorting)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134.34</td>\n",
       "      <td>137.66</td>\n",
       "      <td>117.33</td>\n",
       "      <td>125.07</td>\n",
       "      <td>119.14</td>\n",
       "      <td>130.42</td>\n",
       "      <td>113.54600</td>\n",
       "      <td>119.8914</td>\n",
       "      <td>138.72</td>\n",
       "      <td>112.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116.64</td>\n",
       "      <td>137.53</td>\n",
       "      <td>134.34</td>\n",
       "      <td>125.71</td>\n",
       "      <td>111.14</td>\n",
       "      <td>121.18</td>\n",
       "      <td>131.81000</td>\n",
       "      <td>134.2400</td>\n",
       "      <td>107.08</td>\n",
       "      <td>108.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124.23</td>\n",
       "      <td>153.36</td>\n",
       "      <td>144.96</td>\n",
       "      <td>109.04</td>\n",
       "      <td>129.84</td>\n",
       "      <td>126.53</td>\n",
       "      <td>141.31660</td>\n",
       "      <td>140.8151</td>\n",
       "      <td>111.11</td>\n",
       "      <td>117.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>117.37</td>\n",
       "      <td>126.37</td>\n",
       "      <td>120.73</td>\n",
       "      <td>105.60</td>\n",
       "      <td>119.76</td>\n",
       "      <td>107.04</td>\n",
       "      <td>119.24340</td>\n",
       "      <td>119.8459</td>\n",
       "      <td>112.27</td>\n",
       "      <td>130.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>107.14</td>\n",
       "      <td>112.60</td>\n",
       "      <td>115.59</td>\n",
       "      <td>102.89</td>\n",
       "      <td>99.76</td>\n",
       "      <td>113.67</td>\n",
       "      <td>106.91730</td>\n",
       "      <td>123.3551</td>\n",
       "      <td>105.28</td>\n",
       "      <td>117.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>124.93</td>\n",
       "      <td>117.89</td>\n",
       "      <td>103.71</td>\n",
       "      <td>109.96</td>\n",
       "      <td>122.18</td>\n",
       "      <td>112.21</td>\n",
       "      <td>94.23567</td>\n",
       "      <td>112.9724</td>\n",
       "      <td>146.95</td>\n",
       "      <td>104.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>129.94</td>\n",
       "      <td>140.97</td>\n",
       "      <td>125.92</td>\n",
       "      <td>105.63</td>\n",
       "      <td>136.10</td>\n",
       "      <td>126.53</td>\n",
       "      <td>125.39000</td>\n",
       "      <td>123.1400</td>\n",
       "      <td>119.54</td>\n",
       "      <td>121.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>111.07</td>\n",
       "      <td>109.50</td>\n",
       "      <td>107.68</td>\n",
       "      <td>89.89</td>\n",
       "      <td>126.37</td>\n",
       "      <td>124.64</td>\n",
       "      <td>108.69000</td>\n",
       "      <td>106.0900</td>\n",
       "      <td>114.31</td>\n",
       "      <td>96.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>100.46</td>\n",
       "      <td>122.98</td>\n",
       "      <td>133.94</td>\n",
       "      <td>87.10</td>\n",
       "      <td>112.17</td>\n",
       "      <td>123.22</td>\n",
       "      <td>141.31660</td>\n",
       "      <td>122.3772</td>\n",
       "      <td>83.25</td>\n",
       "      <td>108.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>111.34</td>\n",
       "      <td>112.19</td>\n",
       "      <td>110.89</td>\n",
       "      <td>108.93</td>\n",
       "      <td>123.40</td>\n",
       "      <td>99.48</td>\n",
       "      <td>117.15750</td>\n",
       "      <td>104.2794</td>\n",
       "      <td>113.40</td>\n",
       "      <td>108.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>465 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fluid Composite  Total Composite  Crystal Composite  \\\n",
       "1              134.34           137.66             117.33   \n",
       "3              116.64           137.53             134.34   \n",
       "4              124.23           153.36             144.96   \n",
       "6              117.37           126.37             120.73   \n",
       "8              107.14           112.60             115.59   \n",
       "...               ...              ...                ...   \n",
       "1198           124.93           117.89             103.71   \n",
       "1199           129.94           140.97             125.92   \n",
       "1201           111.07           109.50             107.68   \n",
       "1203           100.46           122.98             133.94   \n",
       "1204           111.34           112.19             110.89   \n",
       "\n",
       "      Visual Episodic Memory  Cognitive Flexibility (Card Sort)  \\\n",
       "1                     125.07                             119.14   \n",
       "3                     125.71                             111.14   \n",
       "4                     109.04                             129.84   \n",
       "6                     105.60                             119.76   \n",
       "8                     102.89                              99.76   \n",
       "...                      ...                                ...   \n",
       "1198                  109.96                             122.18   \n",
       "1199                  105.63                             136.10   \n",
       "1201                   89.89                             126.37   \n",
       "1203                   87.10                             112.17   \n",
       "1204                  108.93                             123.40   \n",
       "\n",
       "      Inhibition (Flanker)  Reading Decoding  Vocabulary Comprehension  \\\n",
       "1                   130.42         113.54600                  119.8914   \n",
       "3                   121.18         131.81000                  134.2400   \n",
       "4                   126.53         141.31660                  140.8151   \n",
       "6                   107.04         119.24340                  119.8459   \n",
       "8                   113.67         106.91730                  123.3551   \n",
       "...                    ...               ...                       ...   \n",
       "1198                112.21          94.23567                  112.9724   \n",
       "1199                126.53         125.39000                  123.1400   \n",
       "1201                124.64         108.69000                  106.0900   \n",
       "1203                123.22         141.31660                  122.3772   \n",
       "1204                 99.48         117.15750                  104.2794   \n",
       "\n",
       "      Processing Speed  Working Memory (List Sorting)  \n",
       "1               138.72                         112.89  \n",
       "3               107.08                         108.06  \n",
       "4               111.11                         117.39  \n",
       "6               112.27                         130.38  \n",
       "8               105.28                         117.39  \n",
       "...                ...                            ...  \n",
       "1198            146.95                         104.06  \n",
       "1199            119.54                         121.89  \n",
       "1201            114.31                          96.99  \n",
       "1203             83.25                         108.06  \n",
       "1204            113.40                         108.06  \n",
       "\n",
       "[465 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCP_cog_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of repetitions you want to perform\n",
    "rep = 100\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .66\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from the Thomas Yeo Lab Github: \n",
    "#ThomasYeoLab/CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100, 150, 200, 300, 500, 700, 1000, 10000]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'alpha': alphas}\n",
    "\n",
    "#set x data to be the input variable you want to use\n",
    "X_HCP_m = HCP_surfarea_norm_m\n",
    "X_ABCD_m = ABCD_surfarea_norm_m\n",
    "\n",
    "Y_HCP_m = HCP_cog_m\n",
    "Y_ABCD_m = ABCD_cog_m\n",
    "\n",
    "\n",
    "X_HCP_f = HCP_surfarea_norm_f\n",
    "X_ABCD_f = ABCD_surfarea_norm_f\n",
    "\n",
    "Y_HCP_f = HCP_cog_f\n",
    "Y_ABCD_f = ABCD_cog_f\n",
    "\n",
    "#number of variables you want to predict to be the number of variables stored in the cognition variablse\n",
    "n_cog = Y_HCP_m.shape[1]\n",
    "\n",
    "#number of features \n",
    "n_feat = X_HCP_m.shape[1]\n",
    "\n",
    "#number of test sets\n",
    "n_test = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrays to store variables\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_ABCD_m = np.zeros([rep,n_cog,n_test])\n",
    "#explained variance\n",
    "var_ABCD_m = np.zeros([rep,n_cog,n_test])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_ABCD_m = np.zeros([rep,n_cog,n_test])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha_ABCD_m = np.zeros([rep,n_cog])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp_ABCD_m = np.zeros([rep,n_feat,n_cog])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe_ABCD_m = np.zeros([rep,n_feat,n_cog])\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_ABCD_f = np.zeros([rep,n_cog,n_test])\n",
    "#explained variance\n",
    "var_ABCD_f = np.zeros([rep,n_cog,n_test])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_ABCD_f = np.zeros([rep,n_cog,n_test])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha_ABCD_f = np.zeros([rep,n_cog])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp_ABCD_f = np.zeros([rep,n_feat,n_cog])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe_ABCD_f = np.zeros([rep,n_feat,n_cog])\n",
    "\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_HCP_m = np.zeros([rep,n_cog,n_test])\n",
    "#explained variance\n",
    "var_HCP_m = np.zeros([rep,n_cog,n_test])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_HCP_m = np.zeros([rep,n_cog,n_test])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha_HCP_m = np.zeros([rep,n_cog])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp_HCP_m = np.zeros([rep,n_feat,n_cog])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe_HCP_m = np.zeros([rep,n_feat,n_cog])\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_HCP_f = np.zeros([rep,n_cog,n_test])\n",
    "#explained variance\n",
    "var_HCP_f = np.zeros([rep,n_cog,n_test])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_HCP_f = np.zeros([rep,n_cog,n_test])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha_HCP_f = np.zeros([rep,n_cog])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp_HCP_f = np.zeros([rep,n_feat,n_cog])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe_HCP_f = np.zeros([rep,n_feat,n_cog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through number of models\n",
    "for p in range(rep):\n",
    "    #print model # you're on\n",
    "    print('Model %d' %(p+1))\n",
    "    #for HCP males\n",
    "    #group split HCP male data into train and test sets, using family ID as group ccategory\n",
    "    train_inds_HCP_m, test_inds_HCP_m = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_HCP_m, groups=HCP_fam_m['Family_ID']))\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_HCP_m = X_HCP_m.iloc[train_inds_HCP_m].values\n",
    "    x_test_HCP_m = X_HCP_m.iloc[test_inds_HCP_m].values\n",
    "        \n",
    "    #set y values for based on indices from split    \n",
    "    cog_train_HCP_m = Y_HCP_m.iloc[train_inds_HCP_m].values\n",
    "    cog_test_HCP_m = Y_HCP_m.iloc[test_inds_HCP_m].values\n",
    "    \n",
    "    #for HCP females\n",
    "    #group split HCP female data into train and test sets, using family ID as group ccategory\n",
    "    train_inds_HCP_f, test_inds_HCP_f = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_HCP_f, groups=HCP_fam_f['Family_ID']))\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_HCP_f = X_HCP_f.iloc[train_inds_HCP_f].values\n",
    "    x_test_HCP_f = X_HCP_f.iloc[test_inds_HCP_f].values\n",
    "        \n",
    "    #set y values for based on indices from split      \n",
    "    cog_train_HCP_f = Y_HCP_f.iloc[train_inds_HCP_f].values\n",
    "    cog_test_HCP_f = Y_HCP_f.iloc[test_inds_HCP_f].values\n",
    "    \n",
    "    \n",
    "    #for ABCD males\n",
    "    train_inds_ABCD_m, test_inds_ABCD_m = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_ABCD_m, groups=ABCD_subj_m['site_id_l']))\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_ABCD_m = X_ABCD_m.iloc[train_inds_ABCD_m].values\n",
    "    x_test_ABCD_m = X_ABCD_m.iloc[test_inds_ABCD_m].values\n",
    "        \n",
    "    #set y values for based on indices from split      \n",
    "    cog_train_ABCD_m = Y_ABCD_m.iloc[train_inds_ABCD_m].values\n",
    "    cog_test_ABCD_m = Y_ABCD_m.iloc[test_inds_ABCD_m].values    \n",
    "    \n",
    "    #for ABCD females\n",
    "    train_inds_ABCD_f, test_inds_ABCD_f = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_ABCD_f, groups=ABCD_subj_f['site_id_l']))\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_ABCD_f = X_ABCD_f.iloc[train_inds_ABCD_f].values\n",
    "    x_test_ABCD_f = X_ABCD_f.iloc[test_inds_ABCD_f].values\n",
    "        \n",
    "    #set y values for based on indices from split      \n",
    "    cog_train_ABCD_f = Y_ABCD_f.iloc[train_inds_ABCD_f].values\n",
    "    cog_test_ABCD_f = Y_ABCD_f.iloc[test_inds_ABCD_f].values  \n",
    "    \n",
    "    \n",
    "    #iterate through the cognitive metrics you want to predict\n",
    "    for cog in range (n_cog):\n",
    "\n",
    "        #print and set cognitive metrics being predicted \n",
    "        print (\"Behaviour: %s\" % Y_HCP_m.columns[cog])\n",
    "    \n",
    "        y_train_HCP_m = cog_train_HCP_m[:,cog]\n",
    "        y_test_HCP_m = cog_test_HCP_m[:,cog]\n",
    "        \n",
    "        y_train_HCP_f = cog_train_HCP_f[:,cog]\n",
    "        y_test_HCP_f = cog_test_HCP_f[:,cog]\n",
    "        \n",
    "        y_train_ABCD_m = cog_train_ABCD_m[:,cog]\n",
    "        y_test_ABCD_m = cog_test_ABCD_m[:,cog]\n",
    "        \n",
    "        y_train_ABCD_f = cog_train_ABCD_f[:,cog]\n",
    "        y_test_ABCD_f = cog_test_ABCD_f[:,cog]\n",
    "        \n",
    "        #store all the y_test values in a separate variable that can be accessed later if needed\n",
    "        #cogtest[p,cog,:] = y_test\n",
    "\n",
    "        ##HCP\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores_HCP_m = []\n",
    "        best_params_HCP_m = []\n",
    "        \n",
    "        best_scores_HCP_f = []\n",
    "        best_params_HCP_f = []\n",
    "        \n",
    "\n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "            \n",
    "            \n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch_HCP_m = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "        #fit regressor to the model, use family ID as group category again\n",
    "        gridSearch_HCP_m.fit(x_train_HCP_m, y_train_HCP_m, groups=HCP_fam_m.iloc[train_inds_HCP_m]['Family_ID'])\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params_HCP_m.append(list(gridSearch_HCP_m.best_params_.values()))\n",
    "        best_scores_HCP_m.append(gridSearch_HCP_m.best_score_)\n",
    "        \n",
    "        \n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch_HCP_f = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "        #fit regressor to the model, use family ID as group category again\n",
    "        gridSearch_HCP_f.fit(x_train_HCP_f, y_train_HCP_f, groups=HCP_fam_f.iloc[train_inds_HCP_f]['Family_ID'])\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params_HCP_f.append(list(gridSearch_HCP_f.best_params_.values()))\n",
    "        best_scores_HCP_f.append(gridSearch_HCP_f.best_score_)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##ABCD\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores_ABCD_m = []\n",
    "        best_params_ABCD_m = []        \n",
    "        \n",
    "        best_scores_ABCD_f = []\n",
    "        best_params_ABCD_f = []        \n",
    "        \n",
    "        \n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "            \n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch_ABCD_m = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "        #fit regressor to the model, use family ID as group category again\n",
    "        gridSearch_ABCD_m.fit(x_train_ABCD_m, y_train_ABCD_m, groups=ABCD_subj_m.iloc[train_inds_ABCD_m]['site_id_l'])\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params_ABCD_m.append(list(gridSearch_ABCD_m.best_params_.values()))\n",
    "        best_scores_ABCD_m.append(gridSearch_ABCD_m.best_score_)\n",
    "        \n",
    "        \n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch_ABCD_f = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "        #fit regressor to the model, use family ID as group category again\n",
    "        gridSearch_ABCD_f.fit(x_train_ABCD_f, y_train_ABCD_f, groups=ABCD_subj_f.iloc[train_inds_ABCD_f]['site_id_l'])\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params_ABCD_f.append(list(gridSearch_ABCD_f.best_params_.values()))\n",
    "        best_scores_ABCD_f.append(gridSearch_ABCD_f.best_score_)\n",
    "\n",
    "        \n",
    "        \n",
    "        #save optimised alpha values\n",
    "        #opt_alpha[p,cog] = best_params[nested_scores.index(np.max(nested_scores))][0]\n",
    "        #ends up just being a single value that's chosen by GridSearchCV here since it's no longer nested\n",
    "        #but the line below just makes it easier to go back to a nested set-up if needed\n",
    "        opt_alpha_HCP_m[p,cog] = best_params_HCP_m[best_scores_HCP_m.index(np.max(best_scores_HCP_m))][0]\n",
    "        opt_alpha_HCP_f[p,cog] = best_params_HCP_f[best_scores_HCP_f.index(np.max(best_scores_HCP_f))][0]\n",
    "        \n",
    "        opt_alpha_ABCD_m[p,cog] = best_params_ABCD_m[best_scores_ABCD_m.index(np.max(best_scores_ABCD_m))][0]\n",
    "        opt_alpha_ABCD_f[p,cog] = best_params_ABCD_f[best_scores_ABCD_f.index(np.max(best_scores_ABCD_f))][0]\n",
    "\n",
    "        #fit model using optimised hyperparameter\n",
    "        model_HCP_m = Ridge(alpha = opt_alpha_HCP_m[p,cog], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model_HCP_m.fit(x_train_HCP_m, y_train_HCP_m);\n",
    "            \n",
    "        model_HCP_f = Ridge(alpha = opt_alpha_HCP_f[p,cog], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model_HCP_f.fit(x_train_HCP_f, y_train_HCP_f);\n",
    "        \n",
    "        \n",
    "        model_ABCD_m = Ridge(alpha = opt_alpha_ABCD_m[p,cog], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model_ABCD_m.fit(x_train_ABCD_m, y_train_ABCD_m);\n",
    "        \n",
    "        model_ABCD_f = Ridge(alpha = opt_alpha_ABCD_f[p,cog], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model_ABCD_f.fit(x_train_ABCD_f, y_train_ABCD_f);\n",
    "        \n",
    "        \n",
    "        #compute r^2 (coefficient of determination)\n",
    "        r2_HCP_m[p,cog,0]=model_HCP_m.score(x_test_HCP_m,y_test_HCP_m)\n",
    "        r2_HCP_m[p,cog,1]=model_HCP_m.score(x_test_HCP_f,y_test_HCP_f)\n",
    "        r2_HCP_m[p,cog,2]=model_HCP_m.score(x_test_ABCD_m,y_test_ABCD_m)\n",
    "        r2_HCP_m[p,cog,3]=model_HCP_m.score(x_test_ABCD_f,y_test_ABCD_f)\n",
    "        \n",
    "        r2_HCP_f[p,cog,0]=model_HCP_f.score(x_test_HCP_m,y_test_HCP_m)\n",
    "        r2_HCP_f[p,cog,1]=model_HCP_f.score(x_test_HCP_f,y_test_HCP_f)\n",
    "        r2_HCP_f[p,cog,2]=model_HCP_f.score(x_test_ABCD_m,y_test_ABCD_m)\n",
    "        r2_HCP_f[p,cog,3]=model_HCP_f.score(x_test_ABCD_f,y_test_ABCD_f)\n",
    "        \n",
    "        r2_ABCD_m[p,cog,0]=model_ABCD_m.score(x_test_HCP_m,y_test_HCP_m)\n",
    "        r2_ABCD_m[p,cog,1]=model_ABCD_m.score(x_test_HCP_f,y_test_HCP_f)\n",
    "        r2_ABCD_m[p,cog,2]=model_ABCD_m.score(x_test_ABCD_m,y_test_ABCD_m)\n",
    "        r2_ABCD_m[p,cog,3]=model_ABCD_m.score(x_test_ABCD_f,y_test_ABCD_f)\n",
    "        \n",
    "        r2_ABCD_f[p,cog,0]=model_ABCD_f.score(x_test_HCP_m,y_test_HCP_m)\n",
    "        r2_ABCD_f[p,cog,1]=model_ABCD_f.score(x_test_HCP_f,y_test_HCP_f)\n",
    "        r2_ABCD_f[p,cog,2]=model_ABCD_f.score(x_test_ABCD_m,y_test_ABCD_m)\n",
    "        r2_ABCD_f[p,cog,3]=model_ABCD_f.score(x_test_ABCD_f,y_test_ABCD_f)\n",
    "        \n",
    "        \n",
    "        preds_HCP_m = []\n",
    "        preds_HCP_f = []\n",
    "        preds_ABCD_m = []\n",
    "        preds_ABCD_f = []\n",
    "\n",
    "        #generate predictions from HCP m model\n",
    "        preds_HCP_m = model_HCP_m.predict(x_test_HCP_m).ravel()\n",
    "        preds_HCP_f = model_HCP_m.predict(x_test_HCP_f).ravel()\n",
    "        preds_ABCD_m = model_HCP_m.predict(x_test_ABCD_m).ravel()\n",
    "        preds_ABCD_f = model_HCP_m.predict(x_test_ABCD_f).ravel()\n",
    "        \n",
    "        #compute explained variance \n",
    "        var_HCP_m[p,cog,0] = explained_variance_score(y_test_HCP_m, preds_HCP_m)\n",
    "        var_HCP_m[p,cog,1] = explained_variance_score(y_test_HCP_f, preds_HCP_f)\n",
    "        var_HCP_m[p,cog,2] = explained_variance_score(y_test_ABCD_m, preds_ABCD_m)\n",
    "        var_HCP_m[p,cog,3] = explained_variance_score(y_test_ABCD_f, preds_ABCD_f)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_HCP_m[p,cog,0] = np.corrcoef(y_test_HCP_m.ravel(), preds_HCP_m)[1,0]\n",
    "        corr_HCP_m[p,cog,1] = np.corrcoef(y_test_HCP_f.ravel(), preds_HCP_f)[1,0]\n",
    "        corr_HCP_m[p,cog,2] = np.corrcoef(y_test_ABCD_m.ravel(), preds_ABCD_m)[1,0]\n",
    "        corr_HCP_m[p,cog,3] = np.corrcoef(y_test_ABCD_f.ravel(), preds_ABCD_f)[1,0]\n",
    "        \n",
    "        preds_HCP_m = []\n",
    "        preds_HCP_f = []\n",
    "        preds_ABCD_m = []\n",
    "        preds_ABCD_f = []\n",
    "\n",
    "        #generate predictions from HCP f model\n",
    "        preds_HCP_m = model_HCP_f.predict(x_test_HCP_m).ravel()\n",
    "        preds_HCP_f = model_HCP_f.predict(x_test_HCP_f).ravel()\n",
    "        preds_ABCD_m = model_HCP_f.predict(x_test_ABCD_m).ravel()\n",
    "        preds_ABCD_f = model_HCP_f.predict(x_test_ABCD_f).ravel()\n",
    "        \n",
    "        #compute explained variance \n",
    "        var_HCP_f[p,cog,0] = explained_variance_score(y_test_HCP_m, preds_HCP_m)\n",
    "        var_HCP_f[p,cog,1] = explained_variance_score(y_test_HCP_f, preds_HCP_f)\n",
    "        var_HCP_f[p,cog,2] = explained_variance_score(y_test_ABCD_m, preds_ABCD_m)\n",
    "        var_HCP_f[p,cog,3] = explained_variance_score(y_test_ABCD_f, preds_ABCD_f)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_HCP_f[p,cog,0] = np.corrcoef(y_test_HCP_m.ravel(), preds_HCP_m)[1,0]\n",
    "        corr_HCP_f[p,cog,1] = np.corrcoef(y_test_HCP_f.ravel(), preds_HCP_f)[1,0]\n",
    "        corr_HCP_f[p,cog,2] = np.corrcoef(y_test_ABCD_m.ravel(), preds_ABCD_m)[1,0]\n",
    "        corr_HCP_f[p,cog,3] = np.corrcoef(y_test_ABCD_f.ravel(), preds_ABCD_f)[1,0]\n",
    "        \n",
    "        preds_HCP_m = []\n",
    "        preds_HCP_f = []\n",
    "        preds_ABCD_m = []\n",
    "        preds_ABCD_f = []\n",
    "\n",
    "        #generate predictions from ABCD m model\n",
    "        preds_HCP_m = model_ABCD_m.predict(x_test_HCP_m).ravel()\n",
    "        preds_HCP_f = model_ABCD_m.predict(x_test_HCP_f).ravel()\n",
    "        preds_ABCD_m = model_ABCD_m.predict(x_test_ABCD_m).ravel()\n",
    "        preds_ABCD_f = model_ABCD_m.predict(x_test_ABCD_f).ravel()\n",
    "        \n",
    "        #compute explained variance \n",
    "        var_ABCD_m[p,cog,0] = explained_variance_score(y_test_HCP_m, preds_HCP_m)\n",
    "        var_ABCD_m[p,cog,1] = explained_variance_score(y_test_HCP_f, preds_HCP_f)\n",
    "        var_ABCD_m[p,cog,2] = explained_variance_score(y_test_ABCD_m, preds_ABCD_m)\n",
    "        var_ABCD_m[p,cog,3] = explained_variance_score(y_test_ABCD_f, preds_ABCD_f)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_ABCD_m[p,cog,0] = np.corrcoef(y_test_HCP_m.ravel(), preds_HCP_m)[1,0]\n",
    "        corr_ABCD_m[p,cog,1] = np.corrcoef(y_test_HCP_f.ravel(), preds_HCP_f)[1,0]\n",
    "        corr_ABCD_m[p,cog,2] = np.corrcoef(y_test_ABCD_m.ravel(), preds_ABCD_m)[1,0]\n",
    "        corr_ABCD_m[p,cog,3] = np.corrcoef(y_test_ABCD_f.ravel(), preds_ABCD_f)[1,0]\n",
    "        \n",
    "        \n",
    "        preds_HCP_m = []\n",
    "        preds_HCP_f = []\n",
    "        preds_ABCD_m = []\n",
    "        preds_ABCD_f = []\n",
    "\n",
    "        #generate predictions from HCP f model\n",
    "        preds_HCP_m = model_ABCD_f.predict(x_test_HCP_m).ravel()\n",
    "        preds_HCP_f = model_ABCD_f.predict(x_test_HCP_f).ravel()\n",
    "        preds_ABCD_m = model_ABCD_f.predict(x_test_ABCD_m).ravel()\n",
    "        preds_ABCD_f = model_ABCD_f.predict(x_test_ABCD_f).ravel()\n",
    "        \n",
    "        #compute explained variance \n",
    "        var_ABCD_f[p,cog,0] = explained_variance_score(y_test_HCP_m, preds_HCP_m)\n",
    "        var_ABCD_f[p,cog,1] = explained_variance_score(y_test_HCP_f, preds_HCP_f)\n",
    "        var_ABCD_f[p,cog,2] = explained_variance_score(y_test_ABCD_m, preds_ABCD_m)\n",
    "        var_ABCD_f[p,cog,3] = explained_variance_score(y_test_ABCD_f, preds_ABCD_f)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_ABCD_f[p,cog,0] = np.corrcoef(y_test_HCP_m.ravel(), preds_HCP_m)[1,0]\n",
    "        corr_ABCD_f[p,cog,1] = np.corrcoef(y_test_HCP_f.ravel(), preds_HCP_f)[1,0]\n",
    "        corr_ABCD_f[p,cog,2] = np.corrcoef(y_test_ABCD_m.ravel(), preds_ABCD_m)[1,0]\n",
    "        corr_ABCD_f[p,cog,3] = np.corrcoef(y_test_ABCD_f.ravel(), preds_ABCD_f)[1,0]\n",
    "        \n",
    "        \n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        #extract feature importance\n",
    "        featimp_HCP_m[p,:,cog] = model_HCP_m.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        cov_x = np.cov(np.transpose(x_train_HCP_m))\n",
    "        cov_y = np.cov(y_train_HCP_m)\n",
    "        featimp_haufe_HCP_m[p,:,cog] = np.matmul(cov_x,featimp_HCP_m[p,:,cog])*(1/cov_y)\n",
    "        \n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        #extract feature importance\n",
    "        featimp_HCP_f[p,:,cog] = model_HCP_f.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        cov_x = np.cov(np.transpose(x_train_HCP_f))\n",
    "        cov_y = np.cov(y_train_HCP_f)\n",
    "        featimp_haufe_HCP_f[p,:,cog] = np.matmul(cov_x,featimp_HCP_f[p,:,cog])*(1/cov_y)\n",
    "        \n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        #extract feature importance\n",
    "        featimp_ABCD_m[p,:,cog] = model_ABCD_m.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        cov_x = np.cov(np.transpose(x_train_ABCD_m))\n",
    "        cov_y = np.cov(y_train_ABCD_m)\n",
    "        featimp_haufe_ABCD_m[p,:,cog] = np.matmul(cov_x,featimp_ABCD_m[p,:,cog])*(1/cov_y)\n",
    "        \n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        #extract feature importance\n",
    "        featimp_ABCD_f[p,:,cog] = model_ABCD_f.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        cov_x = np.cov(np.transpose(x_train_ABCD_f))\n",
    "        cov_y = np.cov(y_train_ABCD_f)\n",
    "        featimp_haufe_ABCD_f[p,:,cog] = np.matmul(cov_x,featimp_ABCD_f[p,:,cog])*(1/cov_y)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "base_dir  = '/Users/elvishadhamala/Documents/yale/HCP_ABCD_preds_results'\n",
    "\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_r2_HCP_m.npy'),r2_HCP_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_var_HCP_m.npy'),var_HCP_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_corr_HCP_m.npy'),corr_HCP_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_alpha_HCP_m.npy'),opt_alpha_HCP_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_HCP_m.npy'),featimp_HCP_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_haufe_HCP_m.npy'),featimp_haufe_HCP_m)\n",
    "\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_r2_HCP_f.npy'),r2_HCP_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_var_HCP_f.npy'),var_HCP_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_corr_HCP_f.npy'),corr_HCP_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_alpha_HCP_f.npy'),opt_alpha_HCP_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_HCP_f.npy'),featimp_HCP_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_haufe_HCP_f.npy'),featimp_haufe_HCP_f)\n",
    "\n",
    "\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_r2_ABCD_m.npy'),r2_ABCD_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_var_ABCD_m.npy'),var_ABCD_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_corr_ABCD_m.npy'),corr_ABCD_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_alpha_ABCD_m.npy'),opt_alpha_ABCD_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_ABCD_m.npy'),featimp_ABCD_m)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_haufe_ABCD_m.npy'),featimp_haufe_ABCD_m)\n",
    "\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_r2_ABCD_f.npy'),r2_ABCD_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_var_ABCD_f.npy'),var_ABCD_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_corr_ABCD_f.npy'),corr_ABCD_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_alpha_ABCD_f.npy'),opt_alpha_ABCD_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_ABCD_f.npy'),featimp_ABCD_f)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_haufe_ABCD_f.npy'),featimp_haufe_ABCD_f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
