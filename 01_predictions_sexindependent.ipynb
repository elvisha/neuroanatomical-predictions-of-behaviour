{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys; sys.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, GroupKFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean HCP data\n",
    "# set base dirctories\n",
    "HCP_base_dir   = '/Users/elvishadhamala/Documents/yale/HCP/'\n",
    "\n",
    "# load dataframes for the cortical areal-level properties\n",
    "# suraface area, gray matter volume, and cortical thickness \n",
    "HCP_surf_area_df = pd.read_csv(os.path.join(HCP_base_dir, 'group_net_surfarea_native_freesurfer.csv'))\n",
    "HCP_gray_vol_df = pd.read_csv(os.path.join(HCP_base_dir, 'group_net_grayvol_native_freesurfer.csv'))\n",
    "HCP_thick_df = pd.read_csv(os.path.join(HCP_base_dir, 'group_net_thickavg_native_freesurfer.csv'))\n",
    "\n",
    "# load subj behavioural and family data\n",
    "HCP_subj_data_df = pd.read_csv(os.path.join(HCP_base_dir, 'hcp_behaviour.csv'))\n",
    "HCP_subj_fam_df = pd.read_csv(os.path.join(HCP_base_dir, 'hcp_restricted.csv'))\n",
    "HCP_subj_fs_df = pd.read_csv(os.path.join(HCP_base_dir, 'hcp_freesurfer.csv'))\n",
    "\n",
    "#create a mask for the subject and family data and only keep the ones that have the cortical areal-level properties \n",
    "HCP_mask_data=np.isin(HCP_subj_data_df.Subject, HCP_surf_area_df.id)\n",
    "HCP_subj_data_df = HCP_subj_data_df[HCP_mask_data]\n",
    "\n",
    "HCP_mask_fam=np.isin(HCP_subj_fam_df.Subject, HCP_surf_area_df.id)\n",
    "HCP_subj_fam_df = HCP_subj_fam_df[HCP_mask_fam]\n",
    "\n",
    "HCP_mask_fs=np.isin(HCP_subj_fs_df.Subject, HCP_surf_area_df.id)\n",
    "HCP_subj_fs_df = HCP_subj_fs_df[HCP_mask_fs]\n",
    "\n",
    "\n",
    "HCP_surf_area_df = HCP_surf_area_df.set_index(HCP_subj_data_df.index)\n",
    "HCP_gray_vol_df = HCP_gray_vol_df.set_index(HCP_subj_data_df.index)\n",
    "HCP_thick_df = HCP_thick_df.set_index(HCP_subj_data_df.index)\n",
    "HCP_subj_fs_df = HCP_subj_fs_df.set_index(HCP_subj_data_df.index)\n",
    "\n",
    "\n",
    "HCP_icv = pd.DataFrame(HCP_subj_fs_df.FS_InterCranial_Vol)\n",
    "\n",
    "# drop the id and 'none' columns\n",
    "HCP_surf_area_df = HCP_surf_area_df.drop(columns=['id', 'lh_None', 'rh_None'])\n",
    "HCP_gray_vol_df = HCP_gray_vol_df.drop(columns=['id', 'lh_None', 'rh_None'])\n",
    "HCP_thick_df = HCP_thick_df.drop(columns=['id', 'lh_None', 'rh_None'])\n",
    "\n",
    "\n",
    "\n",
    "HCP_cog = HCP_subj_data_df[[\"CogFluidComp_Unadj\", \"CogEarlyComp_Unadj\", \"CogTotalComp_Unadj\", \"CogCrystalComp_Unadj\",\n",
    "                  \"PicSeq_Unadj\", \"CardSort_Unadj\", \"Flanker_Unadj\", \"PMAT24_A_CR\", \"ReadEng_Unadj\", \n",
    "                  \"PicVocab_Unadj\", \"ProcSpeed_Unadj\", \"DDisc_AUC_40K\", \"VSPLOT_TC\", \"SCPT_SEN\", \"SCPT_SPEC\", \n",
    "                  \"IWRD_TOT\", \"ListSort_Unadj\", \"MMSE_Score\", \"Language_Task_Math_Avg_Difficulty_Level\", \n",
    "                  \"Language_Task_Story_Avg_Difficulty_Level\", \"Relational_Task_Acc\", \"WM_Task_Acc\"]] \n",
    "\n",
    "\n",
    "col_headers_main = ['Fluid Composite', 'Early Composite', 'Total Composite',\n",
    "               'Crystal Composite', 'Visual Episodic Memory', 'Cognitive Flexibility (Card Sort)',\n",
    "               'Inhibition (Flanker)', 'Fluid Intelligence (PMAT)', 'Reading Decoding', \n",
    "               'Vocabulary Comprehension', 'Processing Speed', 'Delay Discounting',\n",
    "               'Spatial Orientation', 'Sustained Attention - Sens.', \n",
    "               'Sustained Attention - Spec.', 'Verbal Episodic Memory', \n",
    "               'Working Memory (List Sorting)', 'Cognitive Status', 'Arithmetic',\n",
    "               'Story Comprehension', 'Relational Processing', 'Working Memory (N-Back)']\n",
    "\n",
    "HCP_cog.columns = col_headers_main\n",
    "\n",
    "HCP_cog = HCP_cog[['Fluid Composite', 'Total Composite', 'Crystal Composite',\n",
    "                     'Visual Episodic Memory', 'Cognitive Flexibility (Card Sort)',\n",
    "                     'Inhibition (Flanker)', 'Reading Decoding', 'Vocabulary Comprehension', \n",
    "                     'Processing Speed', 'Working Memory (List Sorting)']]\n",
    "\n",
    "\n",
    "\n",
    "#get rid of all the subjects with nans\n",
    "HCP_mask = np.asarray([~HCP_cog.isna().any(axis=1)])\n",
    "HCP_surf_area = HCP_surf_area_df[np.transpose(HCP_mask==True)]\n",
    "HCP_gray_vol = HCP_gray_vol_df[np.transpose(HCP_mask==True)]\n",
    "HCP_thick = HCP_thick_df[np.transpose(HCP_mask==True)]\n",
    "HCP_icv = HCP_icv[np.transpose(HCP_mask==True)]\n",
    "HCP_cog = HCP_cog[np.transpose(HCP_mask==True)]\n",
    "HCP_fam = HCP_subj_fam_df.loc[np.transpose(HCP_mask==True)]\n",
    "HCP_subj = HCP_subj_data_df.loc[np.transpose(HCP_mask==True)]\n",
    "\n",
    "# get normalised measured (by icv)\n",
    "HCP_surf_area_norm = pd.DataFrame(HCP_surf_area.values/HCP_icv.values, columns=HCP_surf_area.columns)\n",
    "HCP_gray_vol_norm = pd.DataFrame(HCP_gray_vol.values/HCP_icv.values, columns=HCP_gray_vol.columns)\n",
    "HCP_thick_norm = pd.DataFrame(HCP_thick.values/HCP_icv.values, columns=HCP_thick.columns)\n",
    "\n",
    "\n",
    "HCP_surfarea = HCP_surf_area.set_index(HCP_subj.index)\n",
    "HCP_grayvol = HCP_gray_vol.set_index(HCP_subj.index)\n",
    "HCP_thick = HCP_thick.set_index(HCP_subj.index)\n",
    "HCP_surfarea_norm = HCP_surf_area_norm.set_index(HCP_subj.index)\n",
    "HCP_grayvol_norm = HCP_gray_vol_norm.set_index(HCP_subj.index)\n",
    "HCP_thick_norm = HCP_thick_norm.set_index(HCP_subj.index)\n",
    "HCP_fam = HCP_fam.set_index(HCP_subj.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean up ABCD data\n",
    "# set base dirctories\n",
    "ABCD_base_dir   = '/Users/elvishadhamala/Documents/yale/ABCD'\n",
    "\n",
    "# load dataframes for the cortical areal-level properties\n",
    "# suraface area, gray matter volume, and cortical thickness \n",
    "ABCD_surf_area_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_group_surfarea.csv'), header=None)\n",
    "ABCD_gray_vol_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_group_grayvol.csv'), header=None)\n",
    "ABCD_thick_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_group_thickavg.csv'), header=None)\n",
    "ABCD_icv_df = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_icv.csv'), header=None)\n",
    "\n",
    "# load subj behavioural and family data\n",
    "ABCD_subj = pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_1823_demo_cog.csv'))\n",
    "\n",
    "\n",
    "ABCD_surfarea = ABCD_surf_area_df.T\n",
    "ABCD_grayvol = ABCD_gray_vol_df.T\n",
    "ABCD_thick = ABCD_thick_df.T\n",
    "\n",
    "\n",
    "# get normalised measured (by icv)\n",
    "ABCD_surfarea_norm = pd.DataFrame(ABCD_surfarea.values/ABCD_icv_df.values, columns=ABCD_surfarea.columns)\n",
    "ABCD_grayvol_norm = pd.DataFrame(ABCD_grayvol.values/ABCD_icv_df.values, columns=ABCD_grayvol.columns)\n",
    "ABCD_thick_norm = pd.DataFrame(ABCD_thick.values/ABCD_icv_df.values, columns=ABCD_thick.columns)\n",
    "\n",
    "\n",
    "ABCD_cog = ABCD_subj\n",
    "ABCD_cog = ABCD_cog.drop(columns=['subjectkey', 'src_subject_id', 'sex', 'race_ethnicity', 'site_id_l'])\n",
    "ABCD_cog\n",
    "\n",
    "col_headers_main = ['Vocabulary Comprehension', 'Inhibition (Flanker)', 'Working Memory (List Sorting)',\n",
    "                   'Cognitive Flexibility (Card Sort)', 'Processing Speed', 'Visual Episodic Memory',\n",
    "                   'Reading Decoding', 'Fluid Composite', 'Crystal Composite', 'Total Composite',\n",
    "                   'RAVLT - Trial VI Correct', 'RAVLT - Trial VII Correct', 'WISC-V - Total Raw Score',\n",
    "                   'LMT - % Correct', 'LMT - RT Correct', 'LMT Efficiency']\n",
    "\n",
    "ABCD_cog.columns = col_headers_main\n",
    "\n",
    "ABCD_cog = ABCD_cog[['Fluid Composite', 'Total Composite', 'Crystal Composite',\n",
    "                     'Visual Episodic Memory', 'Cognitive Flexibility (Card Sort)',\n",
    "                     'Inhibition (Flanker)', 'Reading Decoding', 'Vocabulary Comprehension', \n",
    "                     'Processing Speed', 'Working Memory (List Sorting)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fluid Composite</th>\n",
       "      <th>Total Composite</th>\n",
       "      <th>Crystal Composite</th>\n",
       "      <th>Visual Episodic Memory</th>\n",
       "      <th>Cognitive Flexibility (Card Sort)</th>\n",
       "      <th>Inhibition (Flanker)</th>\n",
       "      <th>Reading Decoding</th>\n",
       "      <th>Vocabulary Comprehension</th>\n",
       "      <th>Processing Speed</th>\n",
       "      <th>Working Memory (List Sorting)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>87.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>83.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>101.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>97.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1823 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fluid Composite  Total Composite  Crystal Composite  \\\n",
       "0                90.0             86.0               87.0   \n",
       "1                77.0             75.0               81.0   \n",
       "2               104.0             92.0               83.0   \n",
       "3                88.0             93.0              101.0   \n",
       "4                84.0             83.0               89.0   \n",
       "...               ...              ...                ...   \n",
       "1818             88.0             88.0               94.0   \n",
       "1819             87.0             83.0               86.0   \n",
       "1820             83.0             85.0               92.0   \n",
       "1821            101.0             93.0               89.0   \n",
       "1822             97.0             91.0               90.0   \n",
       "\n",
       "      Visual Episodic Memory  Cognitive Flexibility (Card Sort)  \\\n",
       "0                      103.0                               93.0   \n",
       "1                      116.0                               84.0   \n",
       "2                      109.0                               95.0   \n",
       "3                       97.0                               97.0   \n",
       "4                       88.0                               96.0   \n",
       "...                      ...                                ...   \n",
       "1818                    95.0                               90.0   \n",
       "1819                   100.0                               85.0   \n",
       "1820                   124.0                               86.0   \n",
       "1821                   111.0                               98.0   \n",
       "1822                    95.0                              102.0   \n",
       "\n",
       "      Inhibition (Flanker)  Reading Decoding  Vocabulary Comprehension  \\\n",
       "0                     84.0              96.0                      81.0   \n",
       "1                     86.0              86.0                      79.0   \n",
       "2                    100.0              91.0                      78.0   \n",
       "3                     91.0              97.0                     105.0   \n",
       "4                     92.0              94.0                      86.0   \n",
       "...                    ...               ...                       ...   \n",
       "1818                  94.0             100.0                      89.0   \n",
       "1819                  85.0              89.0                      86.0   \n",
       "1820                  75.0              95.0                      91.0   \n",
       "1821                 102.0              88.0                      92.0   \n",
       "1822                  99.0              91.0                      90.0   \n",
       "\n",
       "      Processing Speed  Working Memory (List Sorting)  \n",
       "0                 94.0                           94.0  \n",
       "1                 63.0                           74.0  \n",
       "2                101.0                          113.0  \n",
       "3                 71.0                          105.0  \n",
       "4                 82.0                           90.0  \n",
       "...                ...                            ...  \n",
       "1818              92.0                           90.0  \n",
       "1819              88.0                          101.0  \n",
       "1820              74.0                           86.0  \n",
       "1821              99.0                           97.0  \n",
       "1822             115.0                           82.0  \n",
       "\n",
       "[1823 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of repetitions you want to perform\n",
    "rep = 10\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .66\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from the Thomas Yeo Lab Github: \n",
    "#ThomasYeoLab/CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100, 150, 200, 300, 500, 700, 1000, 10000]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'alpha': alphas}\n",
    "\n",
    "#set x data to be the input variable you want to use\n",
    "X_HCP = HCP_surfarea_norm\n",
    "X_ABCD = ABCD_surfarea_norm\n",
    "\n",
    "Y_HCP = HCP_cog\n",
    "Y_ABCD = ABCD_cog\n",
    "\n",
    "#todo: thick, thick_norm\n",
    "\n",
    "\n",
    "#number of variables you want to predict to be the number of variables stored in the cognition variablse\n",
    "n_cog = Y_HCP.shape[1]\n",
    "\n",
    "#number of features \n",
    "n_feat = X_HCP.shape[1]\n",
    "\n",
    "#number of test sets\n",
    "n_test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrays to store variables\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_ABCD = np.zeros([rep,n_cog,n_test])\n",
    "#explained variance\n",
    "var_ABCD = np.zeros([rep,n_cog,n_test])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_ABCD = np.zeros([rep,n_cog,n_test])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha_ABCD = np.zeros([rep,n_cog])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp_ABCD = np.zeros([rep,n_feat,n_cog])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe_ABCD = np.zeros([rep,n_feat,n_cog])\n",
    "\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_HCP = np.zeros([rep,n_cog,n_test])\n",
    "#explained variance\n",
    "var_HCP = np.zeros([rep,n_cog,n_test])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_HCP = np.zeros([rep,n_cog,n_test])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha_HCP = np.zeros([rep,n_cog])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp_HCP = np.zeros([rep,n_feat,n_cog])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe_HCP = np.zeros([rep,n_feat,n_cog])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#iterate through number of models\n",
    "for p in range(rep):\n",
    "    #print model # you're on\n",
    "    print('Model %d' %(p+1))\n",
    "    #HCP\n",
    "    #group split HCP male data into train and test sets, using family ID as group ccategory\n",
    "    train_inds_HCP, test_inds_HCP = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_HCP, groups=HCP_fam['Family_ID']))\n",
    "    #x_train, x_test, cog_train, cog_test = train_test_split(X, Y, test_size=1-train_size, shuffle=True, random_state=p)\n",
    "    \n",
    "    #set x values based on indices from split  \n",
    "    x_train_HCP = X_HCP.iloc[train_inds_HCP].values\n",
    "    x_test_HCP = X_HCP.iloc[test_inds_HCP].values\n",
    "        \n",
    "    #set y values based on indices from split      \n",
    "    cog_train_HCP = Y_HCP.iloc[train_inds_HCP].values\n",
    "    cog_test_HCP = Y_HCP.iloc[test_inds_HCP].values\n",
    "    \n",
    "    \n",
    "    #ABCD\n",
    "    train_inds_ABCD, test_inds_ABCD = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_ABCD, groups=ABCD_subj['site_id_l']))\n",
    "    #x_train, x_test, cog_train, cog_test = train_test_split(X, Y, test_size=1-train_size, shuffle=True, random_state=p)\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_ABCD = X_ABCD.iloc[train_inds_ABCD].values\n",
    "    x_test_ABCD = X_ABCD.iloc[test_inds_ABCD].values\n",
    "        \n",
    "    #set y values based on indices from split  \n",
    "    cog_train_ABCD = Y_ABCD.iloc[train_inds_ABCD].values\n",
    "    cog_test_ABCD = Y_ABCD.iloc[test_inds_ABCD].values    \n",
    "    \n",
    "   \n",
    "    \n",
    "    #iterate through the cognitive metrics you want to predict\n",
    "    for cog in range (n_cog):\n",
    "\n",
    "        #print and set cognitive metrics being predicted \n",
    "        print (\"Behaviour: %s\" % Y_HCP.columns[cog])\n",
    "    \n",
    "        y_train_HCP = cog_train_HCP[:,cog]\n",
    "        y_test_HCP = cog_test_HCP[:,cog]\n",
    "        \n",
    "        y_train_ABCD = cog_train_ABCD[:,cog]\n",
    "        y_test_ABCD = cog_test_ABCD[:,cog]\n",
    "\n",
    "        \n",
    "        #store all the y_test values in a separate variable that can be accessed later if needed\n",
    "        #cogtest[p,cog,:] = y_test\n",
    "\n",
    "        ##HCP\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores_HCP = []\n",
    "        best_params_HCP = []\n",
    "\n",
    "\n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "            \n",
    "            \n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch_HCP = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "        #fit regressor to the model, use family ID as group category again\n",
    "        gridSearch_HCP.fit(x_train_HCP, y_train_HCP, groups=HCP_fam.iloc[train_inds_HCP]['Family_ID'])\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params_HCP.append(list(gridSearch_HCP.best_params_.values()))\n",
    "        best_scores_HCP.append(gridSearch_HCP.best_score_)\n",
    "        \n",
    "        \n",
    "        ##ABCD\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores_ABCD = []\n",
    "        best_params_ABCD = []        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "            \n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch_ABCD = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "        #fit regressor to the model, use family ID as group category again\n",
    "        gridSearch_ABCD.fit(x_train_ABCD, y_train_ABCD, groups=ABCD_subj.iloc[train_inds_ABCD]['site_id_l'])\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params_ABCD.append(list(gridSearch_ABCD.best_params_.values()))\n",
    "        best_scores_ABCD.append(gridSearch_ABCD.best_score_)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #save optimised alpha values\n",
    "        #opt_alpha[p,cog] = best_params[nested_scores.index(np.max(nested_scores))][0]\n",
    "        #ends up just being a single value that's chosen by GridSearchCV here since it's no longer nested\n",
    "        #but the line below just makes it easier to go back to a nested set-up if needed\n",
    "        opt_alpha_HCP[p,cog] = best_params_HCP[best_scores_HCP.index(np.max(best_scores_HCP))][0]\n",
    "        \n",
    "        opt_alpha_ABCD[p,cog] = best_params_ABCD[best_scores_ABCD.index(np.max(best_scores_ABCD))][0]\n",
    "\n",
    "        #fit model using optimised hyperparameter\n",
    "        model_HCP = Ridge(alpha = opt_alpha_HCP[p,cog], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model_HCP.fit(x_train_HCP, y_train_HCP);\n",
    "            \n",
    "      \n",
    "        model_ABCD = Ridge(alpha = opt_alpha_ABCD[p,cog], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model_ABCD.fit(x_train_ABCD, y_train_ABCD);\n",
    "        \n",
    "        \n",
    "        #compute r^2 (coefficient of determination)\n",
    "        r2_HCP[p,cog,0]=model_HCP.score(x_test_HCP,y_test_HCP)\n",
    "        r2_HCP[p,cog,1]=model_HCP.score(x_test_ABCD,y_test_ABCD)\n",
    "        \n",
    "        \n",
    "        r2_ABCD[p,cog,0]=model_ABCD.score(x_test_HCP,y_test_HCP)\n",
    "        r2_ABCD[p,cog,1]=model_ABCD.score(x_test_ABCD,y_test_ABCD)\n",
    "        \n",
    "        \n",
    "        preds_HCP = []\n",
    "        preds_ABCD = []\n",
    "\n",
    "        #generate predictions from HCP m model\n",
    "        preds_HCP = model_HCP.predict(x_test_HCP).ravel()\n",
    "        preds_ABCD = model_HCP.predict(x_test_ABCD).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var_HCP[p,cog,0] = explained_variance_score(y_test_HCP, preds_HCP)\n",
    "        var_HCP[p,cog,1] = explained_variance_score(y_test_ABCD, preds_ABCD)\n",
    "\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_HCP[p,cog,0] = np.corrcoef(y_test_HCP.ravel(), preds_HCP)[1,0]\n",
    "        corr_HCP[p,cog,1] = np.corrcoef(y_test_ABCD.ravel(), preds_ABCD)[1,0]\n",
    "\n",
    "        \n",
    "\n",
    "        preds_HCP = []\n",
    "        preds_ABCD = []\n",
    "\n",
    "        #generate predictions from ABCD m model\n",
    "        preds_HCP = model_ABCD.predict(x_test_HCP).ravel()\n",
    "        preds_ABCD = model_ABCD.predict(x_test_ABCD).ravel()\n",
    "        \n",
    "        #compute explained variance \n",
    "        var_ABCD[p,cog,0] = explained_variance_score(y_test_HCP, preds_HCP)\n",
    "        var_ABCD[p,cog,1] = explained_variance_score(y_test_ABCD, preds_ABCD)\n",
    "\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_ABCD[p,cog,0] = np.corrcoef(y_test_HCP.ravel(), preds_HCP)[1,0]\n",
    "        corr_ABCD[p,cog,1] = np.corrcoef(y_test_ABCD.ravel(), preds_ABCD)[1,0]\n",
    "\n",
    "\n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        #extract feature importance\n",
    "        featimp_HCP[p,:,cog] = model_HCP.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        cov_x = np.cov(np.transpose(x_train_HCP))\n",
    "        cov_y = np.cov(y_train_HCP)\n",
    "        featimp_haufe_HCP[p,:,cog] = np.matmul(cov_x,featimp_HCP[p,:,cog])*(1/cov_y)\n",
    "    \n",
    "        \n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        #extract feature importance\n",
    "        featimp_ABCD[p,:,cog] = model_ABCD.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        cov_x = np.cov(np.transpose(x_train_ABCD))\n",
    "        cov_y = np.cov(y_train_ABCD)\n",
    "        featimp_haufe_ABCD[p,:,cog] = np.matmul(cov_x,featimp_ABCD[p,:,cog])*(1/cov_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "base_dir  = '/Users/elvishadhamala/Documents/yale/HCP_ABCD_preds_results'\n",
    "\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_r2_HCP.npy'),r2_HCP)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_var_HCP.npy'),var_HCP)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_corr_HCP.npy'),corr_HCP)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_alpha_HCP.npy'),opt_alpha_HCP)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_HCP.npy'),featimp_HCP)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_haufe_HCP.npy'),featimp_haufe_HCP)\n",
    "\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_r2_ABCD.npy'),r2_ABCD)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_var_ABCD.npy'),var_ABCD)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_corr_ABCD.npy'),corr_ABCD)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_alpha_ABCD.npy'),opt_alpha_ABCD)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_ABCD.npy'),featimp_ABCD)\n",
    "np.save(os.path.join(base_dir, 'surfarea_norm_featimp_haufe_ABCD.npy'),featimp_haufe_ABCD)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
